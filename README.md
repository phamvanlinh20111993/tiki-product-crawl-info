
# Web Crawler (Learning Golang)

## ğŸ“Œ Introduction
This project is a **simple web crawler** built for the purpose of **learning Golang**.  
The goal is to practice working with:

- HTTP requests
- Parsing web pages
- Handling errors properly
- Structuring Go projects
- Understanding concurrency (future improvement)

This project is **for educational purposes only**.

---

## ğŸ¯ Purpose
- Learn Golang syntax and best practices
- Understand how to crawl and extract data from web pages
- Practice working with APIs, HTML parsing, and JSON
- Improve error handling and clean code in Go

---

## ğŸš€ Features
- Send HTTP requests to web pages
- Crawl and extract basic data
- Handle relative and absolute URLs
- Generic API response handling (using Go generics)
- Clean and simple project structure

---

## ğŸ›  Tech Stack
- **Language:** Golang
- **HTTP Client:** `net/http`
- **Parsing:** `encoding/json` / `goquery` (if applicable)

---

## âš ï¸ Disclaimer
This project is created **only for learning purposes**.  
Please make sure to:
- Respect websites' `robots.txt`
- Follow website terms of service
- Avoid sending excessive requests

---

## ğŸ“‚ Project Structure
```text
.
â”œâ”€â”€ main.go
â”œâ”€â”€ configuration/
â”œâ”€â”€ datasource/
â”œâ”€â”€ handle/
â”œâ”€â”€ http-request
â”œâ”€â”€ logger
â”œâ”€â”€ metadata
â”œâ”€â”€ parser
â”œâ”€â”€ utils
â”œâ”€â”€ crawl-config.yml
â””â”€â”€ README.md

